{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8802127,"sourceType":"datasetVersion","datasetId":5293408},{"sourceId":8811466,"sourceType":"datasetVersion","datasetId":5300160},{"sourceId":8824845,"sourceType":"datasetVersion","datasetId":5309304},{"sourceId":8840038,"sourceType":"datasetVersion","datasetId":5320122}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport random\n\n# Configuration\nmanifest_file = '/kaggle/input/nemo-cleaned-arabic-json/train.json'\ntrain_ratio = 0.8  # 80% for training\ndev_ratio = 0.1    # 10% for development\ntest_ratio = 0.1   # 10% for testing\n\n# Load existing manifest data\nmanifest_data = []\nif os.path.isfile(manifest_file):\n    with open(manifest_file, 'r') as json_file:\n        for line in json_file:\n            manifest_data.append(json.loads(line))\n\n# Shuffle the data\nrandom.shuffle(manifest_data)\n\n# Calculate split indices\ntotal_samples = len(manifest_data)\ntrain_end = int(total_samples * train_ratio)\ndev_end = train_end + int(total_samples * dev_ratio)\n\n# Split the data\ntrain_data = manifest_data[:train_end]\ndev_data = manifest_data[train_end:dev_end]\ntest_data = manifest_data[dev_end:]\n\n# Define a function to write a subset to a JSON file\ndef write_subset(filename, data):\n    with open(filename, 'w') as json_file:\n        for entry in data:\n            entry['audio_filepath'] = entry['audio_filepath']\n            json.dump(entry, json_file)\n            json_file.write('\\n')\n\n# Write the subsets to their respective files\nwrite_subset('train_manifest.json', train_data)\nwrite_subset('dev_manifest.json', dev_data)\nwrite_subset('test_manifest.json', test_data)\n\nprint(f\"Data split into train ({len(train_data)} samples), dev ({len(dev_data)} samples), and test ({len(test_data)} samples) sets.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-01T14:14:20.124987Z","iopub.execute_input":"2024-07-01T14:14:20.125363Z","iopub.status.idle":"2024-07-01T14:14:21.789012Z","shell.execute_reply.started":"2024-07-01T14:14:20.125327Z","shell.execute_reply":"2024-07-01T14:14:21.787989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nemo_toolkit['asr']","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:14:21.790963Z","iopub.execute_input":"2024-07-01T14:14:21.791328Z","iopub.status.idle":"2024-07-01T14:15:38.491556Z","shell.execute_reply.started":"2024-07-01T14:14:21.791295Z","shell.execute_reply":"2024-07-01T14:15:38.490470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/NeMo.git","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:15:38.492905Z","iopub.execute_input":"2024-07-01T14:15:38.493211Z","iopub.status.idle":"2024-07-01T14:15:39.457015Z","shell.execute_reply.started":"2024-07-01T14:15:38.493182Z","shell.execute_reply":"2024-07-01T14:15:39.455964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the .yaml configuration for Fast Conformer Transducer with Egyptian Dialect\nconfig_data = \"\"\"\nname: \"Conformer-Hybrid-Transducer-CTC-Char-Arabic\"\n\nmodel:\n  sample_rate: &sample_rate 16000\n  compute_eval_loss: false \n  log_prediction: true\n  skip_nan_grad: false\n\n  labels: [\" \", \"ا\", \"ب\", \"ت\", \"ث\", \"ج\", \"ح\", \"خ\", \"د\", \"ذ\", \"ر\", \"ز\", \"س\", \"ش\", \"ص\", \"ض\", \"ط\", \"ظ\", \"ع\", \"غ\", \"ف\", \"ق\", \"ك\", \"ل\", \"م\", \"ن\", \"ه\", \"و\", \"ي\", \"ء\", \"أ\", \"إ\", \"آ\", \"ى\"]\n\n  model_defaults:\n    enc_hidden: ${model.encoder.d_model}\n    pred_hidden: 320\n    joint_hidden: 320\n\n  train_ds:\n    manifest_filepath: \"/kaggle/working/train_manifest.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 8\n    shuffle: true\n    num_workers: 4\n    pin_memory: true\n    trim_silence: false\n    max_duration: 16.0\n    min_duration: 0.1\n    # tarred datasets\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    # bucketing params\n    bucketing_strategy: \"synced_randomized\"\n    bucketing_batch_size: null\n\n  validation_ds:\n    manifest_filepath: \"/kaggle/working/dev_manifest.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 8\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 4\n    pin_memory: true\n\n  test_ds:\n    manifest_filepath: \"/kaggle/working/test_manifest.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 8\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 4\n    pin_memory: true\n\n  preprocessor:\n    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n    sample_rate: *sample_rate\n    normalize: \"per_feature\"\n    window_size: 0.025\n    window_stride: 0.01\n    window: \"hann\"\n    features: 80\n    n_fft: 512\n    frame_splicing: 1\n    dither: 0.00001\n    pad_to: 0\n\n  spec_augment:\n    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n    freq_masks: 2\n    time_masks: 10\n    freq_width: 27\n    time_width: 0.05\n\n  encoder:\n    _target_: nemo.collections.asr.modules.ConformerEncoder\n    feat_in: ${model.preprocessor.features}\n    feat_out: -1 # you may set it if you need different output size other than the default d_model\n    n_layers: 17\n    d_model: 512\n\n    # Sub-sampling params\n    subsampling: striding\n    subsampling_factor: 4\n    subsampling_conv_channels: -1\n    causal_downsampling: false\n\n    reduction: null # pooling, striding, or null\n    reduction_position: null # Encoder block index or -1 for subsampling at the end of encoder\n    reduction_factor: 1\n\n    # Feed forward module's params\n    ff_expansion_factor: 4\n\n    # Multi-headed Attention Module's params\n    self_attention_model: rel_pos # rel_pos or abs_pos\n    n_heads: 8 # may need to be lower for smaller d_models\n    # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention\n    att_context_size: [-1, -1] # -1 means unlimited context\n    att_context_style: regular # regular or chunked_limited\n    xscaling: true # scales up the input embeddings by sqrt(d_model)\n    untie_biases: true # unties the biases of the TransformerXL layers\n    pos_emb_max_len: 5000\n\n    # Convolution module's params\n    conv_kernel_size: 31\n    conv_norm_type: 'batch_norm' # batch_norm or layer_norm or groupnormN (N specifies the number of groups)\n    # conv_context_size can be\"causal\" or a list of two integers while conv_context_size[0]+conv_context_size[1]+1==conv_kernel_size\n    # null means [(kernel_size-1)//2, (kernel_size-1)//2], and 'causal' means [(kernel_size-1), 0]\n    conv_context_size: null\n\n    ### regularization\n    dropout: 0.1 # The dropout used in most of the Conformer Modules\n    dropout_pre_encoder: 0.1 # The dropout used before the encoder\n    dropout_emb: 0.0 # The dropout used for embeddings\n    dropout_att: 0.1 # The dropout for multi-headed attention modules\n\n    # set to non-zero to enable stochastic depth\n    stochastic_depth_drop_prob: 0.0\n    stochastic_depth_mode: linear  # linear or uniform\n    stochastic_depth_start_layer: 1\n\n  decoder:\n    _target_: nemo.collections.asr.modules.RNNTDecoder\n    normalization_mode: null # Currently only null is supported for export.\n    random_state_sampling: false # Random state sampling: https://arxiv.org/pdf/1910.11455.pdf\n    blank_as_pad: true # This flag must be set in order to support exporting of RNNT models + efficient inference.\n\n    prednet:\n      pred_hidden: ${model.model_defaults.pred_hidden}\n      pred_rnn_layers: 1\n      t_max: null\n      dropout: 0.2\n\n  joint:\n    _target_: nemo.collections.asr.modules.RNNTJoint\n    log_softmax: null  # 'null' would set it automatically according to CPU/GPU device\n    preserve_memory: false  # dramatically slows down training, but might preserve some memory\n\n    # Fuses the computation of prediction net + joint net + loss + WER calculation\n    # to be run on sub-batches of size `fused_batch_size`.\n    # When this flag is set to true, consider the `batch_size` of *_ds to be just `encoder` batch size.\n    # `fused_batch_size` is the actual batch size of the prediction net, joint net and transducer loss.\n    # Using small values here will preserve a lot of memory during training, but will make training slower as well.\n    # An optimal ratio of fused_batch_size : *_ds.batch_size is 1:1.\n    # However, to preserve memory, this ratio can be 1:8 or even 1:16.\n    # Extreme case of 1:B (i.e. fused_batch_size=1) should be avoided as training speed would be very slow.\n    fuse_loss_wer: true\n    fused_batch_size: 4\n\n    jointnet:\n      joint_hidden: ${model.model_defaults.joint_hidden}\n      activation: \"relu\"\n      dropout: 0.2\n\n  decoding:\n    strategy: \"greedy_batch\" # can be greedy, greedy_batch, beam, tsd, alsd.\n\n    # greedy strategy config\n    greedy:\n      max_symbols: 10\n\n    # beam strategy config\n    beam:\n      beam_size: 2\n      return_best_hypothesis: False\n      score_norm: true\n      tsd_max_sym_exp: 50  # for Time Synchronous Decoding\n      alsd_max_target_len: 2.0  # for Alignment-Length Synchronous Decoding\n\n  # The section which would contain the decoder and decoding configs of the auxiliary CTC decoder\n  aux_ctc:\n    ctc_loss_weight: 0.3 # the weight used to combine the CTC loss with the RNNT loss\n    use_cer: false\n    ctc_reduction: 'mean_batch'\n    decoder:\n      _target_: nemo.collections.asr.modules.ConvASRDecoder\n      feat_in: null\n      num_classes: -1\n      vocabulary: ${model.labels}\n    decoding:\n      strategy: \"greedy\"\n\n  # config for InterCTC loss: https://arxiv.org/abs/2102.03216\n  # specify loss weights and which layers to use for InterCTC\n  # e.g., to reproduce the paper results, set loss_weights: [0.3]\n  # and apply_at_layers: [8] (assuming 18 layers). Note that final\n  # layer loss coefficient is automatically adjusted (to 0.7 in above example)\n  interctc:\n    loss_weights: []\n    apply_at_layers: []\n\n  loss:\n    loss_name: \"default\"\n\n    warprnnt_numba_kwargs:\n      # FastEmit regularization: https://arxiv.org/abs/2010.11148\n      # You may enable FastEmit to reduce the latency of the model for streaming\n      fastemit_lambda: 0.0  # Recommended values to be in range [1e-4, 1e-2], 0.001 is a good start.\n      clamp: -1.0  # if > 0, applies gradient clamping in range [-clamp, clamp] for the joint tensor only.\n\n  optim:\n    name: adamw\n    lr: 5.0\n    # optimizer arguments\n    betas: [0.9, 0.98]\n    weight_decay: 1e-3\n\n    # scheduler setup\n    sched:\n      name: NoamAnnealing\n      d_model: ${model.encoder.d_model}\n      # scheduler config override\n      warmup_steps: 10000\n      warmup_ratio: null\n      min_lr: 1e-6\n\ntrainer:\n  devices: -1 # number of GPUs, -1 would use all available GPUs\n  num_nodes: 1\n  max_epochs: 10\n  max_steps: -1 # computed at runtime if not set\n  val_check_interval: 1.0 # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n  accelerator: auto\n  strategy: ddp\n  accumulate_grad_batches: 2\n  gradient_clip_val: 0.0\n  precision: 16 # 16, 32, or bf16\n  log_every_n_steps: 10  # Interval of logging.\n  enable_progress_bar: True\n  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it\n  check_val_every_n_epoch: 1 # number of evaluations on validation every n epochs\n  sync_batchnorm: true\n  enable_checkpointing: false  # Provided by exp_manager\n  logger: false  # Provided by exp_manager\n  benchmark: false # needs to be false for models with variable-length speech input as it slows down training\n\n\nexp_manager:\n  exp_dir: 'experiments/'\n  name: ${name}\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  checkpoint_callback_params:\n    # in case of multiple validation sets, first one is used\n    monitor: \"val_wer\"\n    mode: \"min\"\n    save_top_k: 5\n    always_save_nemo: True # saves the checkpoints as nemo files instead of PTL checkpoints\n\n  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.\n  # you need to set these two to True to continue the training\n  resume_if_exists: false\n  resume_ignore_no_checkpoint: false\n\n  # You may use this section to create a W&B logger\n  create_wandb_logger: false\n  wandb_logger_kwargs:\n    name: null\n    project: null\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:26:50.195126Z","iopub.execute_input":"2024-07-01T14:26:50.195572Z","iopub.status.idle":"2024-07-01T14:26:50.209484Z","shell.execute_reply.started":"2024-07-01T14:26:50.195528Z","shell.execute_reply":"2024-07-01T14:26:50.208356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Ensure the directory exists before writing the file\ndirectory = \"/kaggle/working/NeMo/conf\"\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n# Write the config to a .yaml file in the specified directory\nwith open(os.path.join(directory, \"hybrid_transducer_ctc_egyptian_dialect.yaml\"), \"w\") as file:\n    file.write(config_data)\n\nprint(f\"Configuration saved to {os.path.abspath(os.path.join(directory, 'hybrid_transducer_ctc_egyptian_dialect.yaml'))}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:26:50.210861Z","iopub.execute_input":"2024-07-01T14:26:50.211769Z","iopub.status.idle":"2024-07-01T14:26:50.224386Z","shell.execute_reply.started":"2024-07-01T14:26:50.211719Z","shell.execute_reply":"2024-07-01T14:26:50.223506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the directory name\ndirectory = \"train_experiments\"\n\n# Create the directory\nif not os.path.exists(directory):\n    os.makedirs(directory)\n    print(f\"Directory '{directory}' created successfully.\")\nelse:\n    print(f\"Directory '{directory}' already exists.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:26:50.226169Z","iopub.execute_input":"2024-07-01T14:26:50.226829Z","iopub.status.idle":"2024-07-01T14:26:50.233173Z","shell.execute_reply.started":"2024-07-01T14:26:50.226792Z","shell.execute_reply":"2024-07-01T14:26:50.232301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the contents of the new file\nfile_path = \"/kaggle/working/hybrid_transducer_ctc_model.py\"\n\n# Append the new training code to the existing content\ncode = \"\"\"\nimport pytorch_lightning as pl\nfrom omegaconf import OmegaConf\n\nfrom nemo.collections.asr.models import EncDecHybridRNNTCTCModel\nfrom nemo.core.config import hydra_runner\nfrom nemo.utils import logging\nfrom nemo.utils.exp_manager import exp_manager\n\n\n@hydra_runner(config_path=\"../conf/conformer/hybrid_transducer_ctc/\", config_name=\"conformer_hybrid_transducer_ctc\")\ndef main(cfg):\n    logging.info(f'Hydra config: {OmegaConf.to_yaml(cfg)}')\n\n    trainer = pl.Trainer(**cfg.trainer)\n    exp_manager(trainer, cfg.get(\"exp_manager\", None))\n    asr_model = EncDecHybridRNNTCTCModel(cfg=cfg.model, trainer=trainer)\n\n    # Initialize the weights of the model from another model, if provided via config\n    asr_model.maybe_init_from_pretrained_checkpoint(cfg)\n\n    trainer.fit(asr_model)\n    asr_model.save_to('/kaggle/working/train_experiments/hyrbrid-transducer-ctc-v1.nemo')\n    if hasattr(cfg.model, 'test_ds') and cfg.model.test_ds.manifest_filepath is not None:\n        if asr_model.prepare_test(trainer):\n            trainer.test(asr_model)\n\n\nif __name__ == '__main__':\n    main()  # noqa pylint: disable=no-value-for-parameter\n\n\"\"\"\n\n# Write the modified content back to the file\nwith open(file_path, \"w\") as file:\n    file.write(code)\n\nprint(\"File created successfully!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:26:50.234361Z","iopub.execute_input":"2024-07-01T14:26:50.234609Z","iopub.status.idle":"2024-07-01T14:26:50.242356Z","shell.execute_reply.started":"2024-07-01T14:26:50.234588Z","shell.execute_reply":"2024-07-01T14:26:50.241415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/hybrid_transducer_ctc_model.py --config-path='/kaggle/working/NeMo/conf' --config-name='hybrid_transducer_ctc_egyptian_dialect.yaml'","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:26:51.185305Z","iopub.execute_input":"2024-07-01T14:26:51.185653Z","iopub.status.idle":"2024-07-01T14:30:37.719296Z","shell.execute_reply.started":"2024-07-01T14:26:51.185625Z","shell.execute_reply":"2024-07-01T14:30:37.717962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}